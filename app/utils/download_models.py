#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹é¢„ä¸‹è½½è„šæœ¬
ç”¨äºæ„å»º Docker é•œåƒæ—¶é¢„ä¸‹è½½æ‰€æœ‰æ¨¡å‹

æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€ä» ModelScope ä¸‹è½½ï¼Œä½¿ç”¨é»˜è®¤ç¼“å­˜è·¯å¾„ ~/.cache/modelscope
"""

import os
from pathlib import Path

# === Qwen3-ASR æ¨¡å‹é€‰æ‹© ===
# auto = æ£€æµ‹æ˜¾å­˜è‡ªåŠ¨é€‰æ‹© (<48Gç”¨0.6B, >=48Gç”¨1.7B)
# Qwen3-ASR-1.7B = å¼ºåˆ¶ä½¿ç”¨ 1.7B
# Qwen3-ASR-0.6B = å¼ºåˆ¶ä½¿ç”¨ 0.6B
QWEN_ASR_MODEL = os.getenv("QWEN_ASR_MODEL", "auto")


def _get_qwen_models() -> list[tuple[str, str]]:
    """æ ¹æ®é…ç½®è¿”å›è¦ä¸‹è½½çš„ Qwen3-ASR æ¨¡å‹åˆ—è¡¨"""
    model_config = QWEN_ASR_MODEL

    # å¼ºåˆ¶æŒ‡å®šæ¨¡å‹
    if model_config == "Qwen3-ASR-1.7B":
        return [
            ("Qwen/Qwen3-ASR-1.7B", "Qwen3-ASR 1.7B (vLLM åç«¯ï¼Œå¼ºåˆ¶æŒ‡å®š)"),
            ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner 0.6B (æ—¶é—´æˆ³å¯¹é½)"),
        ]
    elif model_config == "Qwen3-ASR-0.6B":
        return [
            ("Qwen/Qwen3-ASR-0.6B", "Qwen3-ASR 0.6B (vLLM åç«¯ï¼Œè½»é‡ç‰ˆï¼Œå¼ºåˆ¶æŒ‡å®š)"),
            ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner 0.6B (æ—¶é—´æˆ³å¯¹é½)"),
        ]
    else:  # auto æˆ–å…¶ä»–å€¼
        try:
            import torch

            if torch.cuda.is_available():
                total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                if total_vram >= 48:
                    print(f"æ£€æµ‹åˆ°æ˜¾å­˜ {total_vram:.1f}GB >= 48GBï¼ŒåŠ è½½ Qwen3-ASR-1.7B")
                    return [
                        ("Qwen/Qwen3-ASR-1.7B", "Qwen3-ASR 1.7B (vLLM åç«¯ï¼Œè‡ªåŠ¨é€‰æ‹©)"),
                        ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner 0.6B (æ—¶é—´æˆ³å¯¹é½)"),
                    ]
                else:
                    print(f"æ£€æµ‹åˆ°æ˜¾å­˜ {total_vram:.1f}GB < 48GBï¼ŒåŠ è½½ Qwen3-ASR-0.6B")
                    return [
                        ("Qwen/Qwen3-ASR-0.6B", "Qwen3-ASR 0.6B (vLLM åç«¯ï¼Œè½»é‡ç‰ˆï¼Œè‡ªåŠ¨é€‰æ‹©)"),
                        ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner 0.6B (æ—¶é—´æˆ³å¯¹é½)"),
                    ]
            else:
                print("æ—  CUDA è®¾å¤‡ï¼Œä¸‹è½½ Qwen3-ASR-0.6B (è½»é‡ç‰ˆ)")
                return [
                    ("Qwen/Qwen3-ASR-0.6B", "Qwen3-ASR 0.6B (vLLM åç«¯ï¼Œè½»é‡ç‰ˆï¼Œæ— GPU)"),
                    ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner 0.6B (æ—¶é—´æˆ³å¯¹é½)"),
                ]
        except ImportError:
            print("æ— æ³•æ£€æµ‹æ˜¾å­˜ï¼Œé»˜è®¤ä¸‹è½½ Qwen3-ASR-1.7B")
            return [
                ("Qwen/Qwen3-ASR-1.7B", "Qwen3-ASR 1.7B (vLLM åç«¯ï¼Œé»˜è®¤)"),
                ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner 0.6B (æ—¶é—´æˆ³å¯¹é½)"),
            ]


# === æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€ä» ModelScope ä¸‹è½½ ===
# æ ‡å‡†ç¼“å­˜è·¯å¾„: ~/.cache/modelscope/hub/models/{model_id}/
ALL_MODELS = [
    # Paraformer æ¨¡å‹
    ("iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch", "Paraformer Large ç¦»çº¿æ¨¡å‹(VAD+æ ‡ç‚¹)"),
    ("iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online", "Paraformer Large å®æ—¶æ¨¡å‹"),
    ("iic/speech_fsmn_vad_zh-cn-16k-common-pytorch", "è¯­éŸ³æ´»åŠ¨æ£€æµ‹æ¨¡å‹(VAD) - iic"),
    ("damo/speech_fsmn_vad_zh-cn-16k-common-pytorch", "è¯­éŸ³æ´»åŠ¨æ£€æµ‹æ¨¡å‹(VAD) - damo"),
    ("iic/speech_campplus_speaker-diarization_common", "è¯´è¯äººåˆ†ç¦»æ¨¡å‹(CAM++)"),
    ("damo/speech_campplus_sv_zh-cn_16k-common", "å£°çº¹è¯†åˆ«æ¨¡å‹(CAM++ä¾èµ–)"),
    ("damo/speech_campplus-transformer_scl_zh-cn_16k-common", "CAM++ transformeræ¨¡å‹(è¯´è¯äººåˆ†ç¦»ä¾èµ–)"),
    ("iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch", "æ ‡ç‚¹ç¬¦å·æ¨¡å‹(ç¦»çº¿)"),
    ("iic/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727", "æ ‡ç‚¹ç¬¦å·æ¨¡å‹(å®æ—¶)"),
    ("iic/speech_ngram_lm_zh-cn-ai-wesp-fst", "è¯­è¨€æ¨¡å‹(N-gram LM)"),
] + _get_qwen_models()


def check_model_exists(model_id: str) -> tuple[bool, str]:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨äºæœ¬åœ°ç¼“å­˜

    æ ‡å‡†è·¯å¾„: ~/.cache/modelscope/hub/models/{model_id}/
    """
    from pathlib import Path

    try:
        cache_dir = Path.home() / ".cache" / "modelscope"
        model_path = cache_dir / "hub" / "models" / model_id

        if model_path.exists() and model_path.is_dir():
            if any(model_path.iterdir()):
                return True, str(model_path)
    except Exception:
        pass

    return False, ""


def check_all_models() -> list[str]:
    """æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ˜¯å¦å­˜åœ¨

    Returns:
        ç¼ºå¤±çš„æ¨¡å‹IDåˆ—è¡¨
    """
    missing = []
    for model_id, _ in ALL_MODELS:
        exists, _ = check_model_exists(model_id)
        if not exists:
            missing.append(model_id)

    return missing


def download_models(auto_mode: bool = False) -> bool:
    """ä¸‹è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹

    Args:
        auto_mode: å¦‚æœä¸ºTrueï¼Œè¡¨ç¤ºè‡ªåŠ¨æ¨¡å¼ï¼ˆä»start.pyè°ƒç”¨ï¼‰ï¼Œä¼šç®€åŒ–è¾“å‡º

    Returns:
        æ˜¯å¦å…¨éƒ¨ä¸‹è½½æˆåŠŸ
    """
    from modelscope.hub.snapshot_download import snapshot_download

    # æ£€æŸ¥ç¼ºå¤±çš„æ¨¡å‹
    missing = check_all_models()

    if not missing:
        if not auto_mode:
            print("âœ… æ‰€æœ‰æ¨¡å‹å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½")
        return True

    cache_dir = Path.home() / ".cache" / "modelscope"

    if auto_mode:
        print(f"ğŸ“¦ æ£€æµ‹åˆ° {len(missing)} ä¸ªæ¨¡å‹éœ€è¦ä¸‹è½½...")
    else:
        print("=" * 60)
        print("FunASR-API æ¨¡å‹é¢„ä¸‹è½½")
        print("=" * 60)
        print(f"ModelScope ç¼“å­˜: {cache_dir}")
        print(f"å¾…ä¸‹è½½æ¨¡å‹: {len(missing)} ä¸ª")
        print("=" * 60)

    failed = []
    downloaded = []

    # ä¸‹è½½æ‰€æœ‰æ¨¡å‹ï¼ˆç»Ÿä¸€ä» ModelScopeï¼‰
    if missing:
        if not auto_mode:
            print("\nğŸ“¦ å¼€å§‹ä¸‹è½½ ModelScope æ¨¡å‹...")
            print("-" * 60)

        for i, (model_id, desc) in enumerate(ALL_MODELS, 1):
            if model_id not in missing:
                continue

            if not auto_mode:
                print(f"\n[{i}/{len(ALL_MODELS)}] {desc}")
                print(f"    æ¨¡å‹ID: {model_id}")
                print(f"    ğŸ“¥ å¼€å§‹ä¸‹è½½...", end="")

            try:
                # ä½¿ç”¨ ModelScope é»˜è®¤ç¼“å­˜è·¯å¾„
                path = snapshot_download(model_id)
                if not auto_mode:
                    print(f" âœ… å®Œæˆ: {path}")
                downloaded.append(model_id)
            except Exception as e:
                if not auto_mode:
                    print(f" âŒ å¤±è´¥: {e}")
                failed.append((model_id, str(e)))

    if not auto_mode:
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"  âœ… å·²ä¸‹è½½: {len(downloaded)} ä¸ª")
        print(f"  âŒ å¤±è´¥: {len(failed)} ä¸ª")
        print("=" * 60)

        if failed:
            print(f"\nå¤±è´¥çš„æ¨¡å‹:")
            for model_id, err in failed:
                print(f"  - {model_id}: {err}")
            return False
        else:
            print("\nâœ… æ‰€æœ‰æ¨¡å‹å‡†å¤‡å°±ç»ª!")
            print("=" * 60)

    return len(failed) == 0


if __name__ == "__main__":
    download_models()

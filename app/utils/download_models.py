#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹é¢„ä¸‹è½½è„šæœ¬
ç”¨äºæ„å»º Docker é•œåƒæ—¶é¢„ä¸‹è½½æ‰€æœ‰æ¨¡å‹

- Paraformer æ¨¡å‹ä» ModelScope ä¸‹è½½
- Qwen3-ASR æ¨¡å‹ä» HuggingFace ä¸‹è½½ (vLLM è¦æ±‚)
"""

import os
from pathlib import Path
from typing import Optional

from huggingface_hub import snapshot_download as hf_snapshot_download
from modelscope.hub.snapshot_download import snapshot_download as ms_snapshot_download

# === Qwen3-ASR æ¨¡å‹é€‰æ‹© ===
QWEN_ASR_MODEL = os.getenv("QWEN_ASR_MODEL", "auto")


def _get_qwen_models() -> list[tuple[str, str]]:
    """è¿”å›è¦ä» HuggingFace ä¸‹è½½çš„ Qwen3-ASR æ¨¡å‹åˆ—è¡¨"""
    config = QWEN_ASR_MODEL

    # å¼ºåˆ¶æŒ‡å®šæ¨¡å‹
    if config == "Qwen3-ASR-1.7B":
        return [
            ("Qwen/Qwen3-ASR-1.7B", "Qwen3-ASR 1.7B"),
            ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner"),
        ]
    elif config == "Qwen3-ASR-0.6B":
        return [
            ("Qwen/Qwen3-ASR-0.6B", "Qwen3-ASR 0.6B"),
            ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner"),
        ]

    # auto æ¨¡å¼
    try:
        import torch
        if torch.cuda.is_available():
            vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if vram >= 32:
                print(f"æ˜¾å­˜ {vram:.1f}GB >= 32GBï¼ŒåŠ è½½ Qwen3-ASR-1.7B")
                return [("Qwen/Qwen3-ASR-1.7B", "Qwen3-ASR 1.7B"), ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner")]
            else:
                print(f"æ˜¾å­˜ {vram:.1f}GB < 32GBï¼ŒåŠ è½½ Qwen3-ASR-0.6B")
                return [("Qwen/Qwen3-ASR-0.6B", "Qwen3-ASR 0.6B"), ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner")]
        else:
            print("æ—  CUDAï¼Œè·³è¿‡ Qwen3-ASRï¼ˆvLLM ä¸æ”¯æŒ CPUï¼‰")
            return []  # CPU æ¨¡å¼ä¸‹ä¸ä¸‹è½½ Qwen æ¨¡å‹
    except ImportError:
        print("é»˜è®¤ä¸‹è½½ Qwen3-ASR-1.7B")
        return [("Qwen/Qwen3-ASR-1.7B", "Qwen3-ASR 1.7B"), ("Qwen/Qwen3-ForcedAligner-0.6B", "Qwen3-ForcedAligner")]


# === ModelScope æ¨¡å‹ (Paraformer) ===
# æ³¨æ„ï¼šä»…åˆ—å‡ºä»£ç ä¸­å®é™…ä½¿ç”¨çš„æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½
# åŒä¸€æ¨¡å‹çš„ä¸åŒå‘½åï¼ˆå¦‚ iic/ vs damo/ï¼‰åªä¿ç•™å®é™…ä½¿ç”¨çš„ç‰ˆæœ¬
# æ ¼å¼: (model_id, description, revision)  revision ä¸º None è¡¨ç¤ºä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
MODELSCOPE_MODELS = [
    ("iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch", "Paraformer Large", None),
    ("iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online", "Paraformer Online", None),
    # VAD æ¨¡å‹: config.py ä¸­ä½¿ç”¨çš„æ˜¯ damo/ ç‰ˆæœ¬
    # CAM++ ä¾èµ– v2.0.2 ç‰ˆæœ¬ï¼Œä¸æ˜¯æœ€æ–°ç‰ˆ
    ("damo/speech_fsmn_vad_zh-cn-16k-common-pytorch", "VAD", "v2.0.2"),
    # CAM++ è¯´è¯äººåˆ†ç¦»: speaker_diarizer.py ä¸­ä½¿ç”¨çš„æ˜¯ iic/speech_campplus_speaker-diarization_common
    # æ³¨æ„ï¼šä»¥ä¸‹ä¸¤ä¸ª damo/ æ¨¡å‹æ˜¯ CAM++ çš„éšå¼ä¾èµ–ï¼ŒFunASR ä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œéœ€é¢„ç½®é¿å…è¿è¡Œæ—¶ä¸‹è½½
    ("iic/speech_campplus_speaker-diarization_common", "CAM++", None),
    ("damo/speech_campplus_sv_zh-cn_16k-common", "CAM++ SV (éšå¼ä¾èµ–)", None),
    ("damo/speech_campplus-transformer_scl_zh-cn_16k-common", "CAM++ Transformer (éšå¼ä¾èµ–)", None),
    ("iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch", "æ ‡ç‚¹æ¨¡å‹", None),
    ("iic/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727", "æ ‡ç‚¹æ¨¡å‹(å®æ—¶)", None),
    ("iic/speech_ngram_lm_zh-cn-ai-wesp-fst", "N-gram LM", None),
]

# === HuggingFace æ¨¡å‹ (Qwen3-ASR) ===
HF_MODELS = _get_qwen_models()

# === åˆå¹¶æ‰€æœ‰æ¨¡å‹ï¼ˆç”¨äºæ£€æŸ¥ï¼‰===
ALL_MODELS = MODELSCOPE_MODELS + HF_MODELS


def _get_cache_path(model_id: str, source: str = "modelscope") -> Path:
    """è·å–æ¨¡å‹ç¼“å­˜è·¯å¾„"""
    if source == "huggingface":
        # HF ç¼“å­˜æ ¼å¼: ~/.cache/huggingface/hub/models--{org}--{model}/
        cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        org, model = model_id.split("/", 1)
        model_path = cache_dir / f"models--{org}--{model}"
    else:
        cache_dir = Path.home() / ".cache" / "modelscope"
        model_path = cache_dir / "hub" / "models" / model_id
    return model_path


def check_model_exists(model_id: str, source: str = "modelscope") -> tuple[bool, str]:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨äºæœ¬åœ°ç¼“å­˜"""
    try:
        model_path = _get_cache_path(model_id, source)

        if model_path.exists() and model_path.is_dir():
            if any(model_path.iterdir()):
                return True, str(model_path)
    except Exception:
        pass

    return False, ""


def check_all_models() -> list[tuple[str, str, str, Optional[str]]]:
    """æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ˜¯å¦å­˜åœ¨

    Returns:
        ç¼ºå¤±çš„æ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (model_id, description, source, revision)
    """
    missing = []

    # æ£€æŸ¥ ModelScope æ¨¡å‹
    for item in MODELSCOPE_MODELS:
        model_id, desc, revision = item
        exists, _ = check_model_exists(model_id, source="modelscope")
        if not exists:
            missing.append((model_id, desc, "modelscope", revision))

    # æ£€æŸ¥ HuggingFace æ¨¡å‹ (HF æ¨¡å‹æš‚ä¸æ”¯æŒæŒ‡å®šç‰ˆæœ¬)
    for model_id, desc in HF_MODELS:
        exists, _ = check_model_exists(model_id, source="huggingface")
        if not exists:
            missing.append((model_id, desc, "huggingface", None))

    return missing


def download_models(auto_mode: bool = False) -> bool:
    """ä¸‹è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹

    Args:
        auto_mode: å¦‚æœä¸ºTrueï¼Œè¡¨ç¤ºè‡ªåŠ¨æ¨¡å¼ï¼ˆä»start.pyè°ƒç”¨ï¼‰ï¼Œä¼šç®€åŒ–è¾“å‡º

    Returns:
        æ˜¯å¦å…¨éƒ¨ä¸‹è½½æˆåŠŸ
    """
    # æ£€æŸ¥ç¼ºå¤±çš„æ¨¡å‹
    missing = check_all_models()

    if not missing:
        if not auto_mode:
            print("âœ… æ‰€æœ‰æ¨¡å‹å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½")
        return True

    ms_cache_dir = Path.home() / ".cache" / "modelscope"
    hf_cache_dir = Path.home() / ".cache" / "huggingface"

    if auto_mode:
        print(f"ğŸ“¦ æ£€æµ‹åˆ° {len(missing)} ä¸ªæ¨¡å‹éœ€è¦ä¸‹è½½...")
    else:
        print("=" * 60)
        print("FunASR-API æ¨¡å‹é¢„ä¸‹è½½")
        print("=" * 60)
        print(f"ModelScope ç¼“å­˜: {ms_cache_dir}")
        print(f"HuggingFace ç¼“å­˜: {hf_cache_dir}")
        print(f"å¾…ä¸‹è½½æ¨¡å‹: {len(missing)} ä¸ª")
        print("=" * 60)

    failed = []
    downloaded = []

    # ä¸‹è½½ ModelScope æ¨¡å‹ (Paraformer)
    ms_missing = [(mid, desc, rev) for mid, desc, src, rev in missing if src == "modelscope"]
    if ms_missing:
        if not auto_mode:
            print("\nğŸ“¦ å¼€å§‹ä¸‹è½½ ModelScope æ¨¡å‹ (Paraformer)...")
            print("-" * 60)

        for i, (model_id, desc, revision) in enumerate(ms_missing, 1):
            if not auto_mode:
                print(f"\n[{i}/{len(ms_missing)}] {desc}")
                print(f"    æ¨¡å‹ID: {model_id}")
                if revision:
                    print(f"    ç‰ˆæœ¬: {revision}")
                print(f"    ğŸ“¥ å¼€å§‹ä¸‹è½½...", end="")

            try:
                # ä¼ é€’ç‰ˆæœ¬å‚æ•°ï¼Œå¦‚æœæŒ‡å®šäº†ç‰ˆæœ¬
                if revision:
                    path = ms_snapshot_download(model_id, revision=revision)
                else:
                    path = ms_snapshot_download(model_id)
                if not auto_mode:
                    print(f" âœ… å®Œæˆ: {path}")
                downloaded.append(model_id)
            except Exception as e:
                if not auto_mode:
                    print(f" âŒ å¤±è´¥: {e}")
                failed.append((model_id, str(e)))

    # ä¸‹è½½ HuggingFace æ¨¡å‹ (Qwen3-ASR)
    hf_missing = [(mid, desc, rev) for mid, desc, src, rev in missing if src == "huggingface"]
    if hf_missing:
        if not auto_mode:
            print("\nğŸ“¦ å¼€å§‹ä¸‹è½½ HuggingFace æ¨¡å‹ (Qwen3-ASR)...")
            print("-" * 60)

        for i, (model_id, desc, _) in enumerate(hf_missing, 1):
            if not auto_mode:
                print(f"\n[{i}/{len(hf_missing)}] {desc}")
                print(f"    æ¨¡å‹ID: {model_id}")
                print(f"    ğŸ“¥ å¼€å§‹ä¸‹è½½...", end="")

            try:
                path = hf_snapshot_download(model_id)
                if not auto_mode:
                    print(f" âœ… å®Œæˆ: {path}")
                downloaded.append(model_id)
            except Exception as e:
                if not auto_mode:
                    print(f" âŒ å¤±è´¥: {e}")
                failed.append((model_id, str(e)))

    if not auto_mode:
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"  âœ… å·²ä¸‹è½½: {len(downloaded)} ä¸ª")
        print(f"  âŒ å¤±è´¥: {len(failed)} ä¸ª")
        print("=" * 60)

        if failed:
            print(f"\nå¤±è´¥çš„æ¨¡å‹:")
            for model_id, err in failed:
                print(f"  - {model_id}: {err}")
            return False
        else:
            print("\nâœ… æ‰€æœ‰æ¨¡å‹å‡†å¤‡å°±ç»ª!")
            print("=" * 60)

    return len(failed) == 0


if __name__ == "__main__":
    download_models()
